{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3524/datasets/1_4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([2, 3])\n",
    "\n",
    "weights = {\n",
    "    'node_0' : np.array([1, 1]),\n",
    "    'node_1' : np.array([-1, 1]),\n",
    "    'output' : np.array([2, -1])\n",
    "}\n",
    "node_0_value = input_data.dot(weights['node_0'])\n",
    "node_1_value = input_data.dot(weights['node_1'])\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "\n",
    "print(hidden_layer_values)\n",
    "\n",
    "output = hidden_layer_values.dot(weights['output'])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Activation functions\n",
    "Activation function allows model to caputre non **linearities**\n",
    "- Tanh\n",
    "- ReLU \n",
    "  <img src=\"https://www.safaribooksonline.com/library/view/python-natural-language/9781787121423/assets/02c4f3a4-8c9b-405a-88bd-47b79e3981dc.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9999092   0.76159416]\n",
      "1.23822425257\n"
     ]
    }
   ],
   "source": [
    "node_0_input = input_data.dot(weights['node_0'])\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "\n",
    "node_1_input = input_data.dot(weights['node_1'])\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "print(hidden_layer_outputs)\n",
    "\n",
    "output = hidden_layer_outputs.dot(weights['output'])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deeper networks\n",
    "- Deep networks internally build representations of pattterns in the data\n",
    "- Partially replace the need for feature engineering \n",
    "- Subsequent layers build increasingly sophisticated representations of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    " 'node_0_0': np.array([2, 4]),\n",
    " 'node_0_1': np.array([ 4, -5]),\n",
    " 'node_1_0': np.array([-1,  2]),\n",
    " 'node_1_1': np.array([1, 2]),\n",
    " 'output': np.array([2, 7])\n",
    "}\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = input_data.dot(weights['node_0_0'])\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = input_data.dot(weights['node_0_1'])\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = hidden_0_outputs.dot(weights['node_1_0'])\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = hidden_0_outputs.dot(weights['node_1_1'])\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = relu(hidden_1_outputs.dot(weights['output']))\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = np.array([3, 5])\n",
    "predict_with_network(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization\n",
    "- Loss function is used to aggregate errors in predictions from many data points into single number\n",
    "- Measure of model's predictive performance\n",
    "- For example MEAN SQUARED ERROR\n",
    "- Goal find the weights that give the lowest value for the loss function\n",
    "- Using gradient descent algorithm to optimize\n",
    "- Setps for GD: \n",
    "    - Start at a random point\n",
    "    - until you are somwhere flat:\n",
    "        - find the slope\n",
    "        - take a step downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1 Pred =  7.25 Error  1.25\n",
      "Step  2 Pred =  6.625 Error  0.625\n",
      "Step  3 Pred =  6.3125 Error  0.3125\n",
      "Step  4 Pred =  6.15625 Error  0.15625\n",
      "Step  5 Pred =  6.078125 Error  0.078125\n",
      "Step  6 Pred =  6.0390625 Error  0.0390625\n",
      "Step  7 Pred =  6.01953125 Error  0.01953125\n",
      "Step  8 Pred =  6.009765625 Error  0.009765625\n",
      "Step  9 Pred =  6.0048828125 Error  0.0048828125\n",
      "Step  10 Pred =  6.00244140625 Error  0.00244140625\n",
      "Step  11 Pred =  6.00122070313 Error  0.001220703125\n",
      "Step  12 Pred =  6.00061035156 Error  0.0006103515625\n",
      "Step  13 Pred =  6.00030517578 Error  0.00030517578125\n",
      "Step  14 Pred =  6.00015258789 Error  0.000152587890625\n",
      "Step  15 Pred =  6.00007629395 Error  7.62939453125e-05\n",
      "Step  16 Pred =  6.00003814697 Error  3.81469726563e-05\n",
      "Step  17 Pred =  6.00001907349 Error  1.90734863281e-05\n",
      "Step  18 Pred =  6.00000953674 Error  9.53674316406e-06\n",
      "Step  19 Pred =  6.00000476837 Error  4.76837158203e-06\n",
      "Step  20 Pred =  6.00000238419 Error  2.38418579102e-06\n"
     ]
    }
   ],
   "source": [
    "def run_one_step_gradient(input_data, weights, target, learning_rate):\n",
    "    preds = input_data.dot(weights)\n",
    "    error = preds - target\n",
    "    gradient = 2 * input_data * error\n",
    "    weights_updated  = weights - learning_rate * gradient\n",
    "    return weights_updated\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "weights = np.array([1, 2])\n",
    "input_data = np.array([3, 4])\n",
    "target = 6\n",
    "learning_rate  = 0.01\n",
    "weights_updated = run_one_step_gradient(input_data, weights, target, learning_rate)\n",
    "for i in range(1, 21):\n",
    "    weights_updated = run_one_step_gradient(input_data, weights_updated, target, learning_rate)\n",
    "    pred = input_data.dot(weights_updated)\n",
    "    print(\"Step \", i, 'Pred = ', pred, 'Error ', pred- target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "- Allows gradient descent to update all weights in neural network by getting gradient ofr all weights\n",
    "- Trying to estimate the slope of the loss function with respect to eaach weight\n",
    "<img src=\"https://i.ytimg.com/vi/An5z8lR8asY/maxresdefault.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation process**\n",
    "- Go back one layer at time \n",
    "- Gradients for weight is product of :\n",
    " - Node value feedingg into that weight\n",
    " - Slope of loss function w.r.t node it feeds into\n",
    " - Slope of activation function at the node it feeds into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "predictors = pd.read_csv('https://assets.datacamp.com/production/course_1975/datasets/hourly_wages.csv')\n",
    "X = predictors.drop(columns=['wage_per_hour']).values\n",
    "y = predictors['wage_per_hour'].values\n",
    "\n",
    "n_cols = X.shape[1] #number of nodes in the input layer\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "#dense layer because all the node in the previous layer are connected to the nodes in the current layer\n",
    "#100 nodes in layer\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=(n_cols, )))\n",
    "\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "#ann_viz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./graphs/nn.PNG\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e12ffa5208>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting (applying back_prob)\n",
    "model.fit(X, y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models\n",
    "- categorical_crosstropy loss function\n",
    "- similar to log loss \n",
    "- uses softmax as activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "df = pd.read_csv('https://assets.datacamp.com/production/course_1975/datasets/titanic_all_numeric.csv')\n",
    "X = df.drop(columns=['survived']).values\n",
    "\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "n_cols = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 343us/step - loss: 2.2923 - acc: 0.6049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e132c140f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential() \n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation=\"relu\", input_shape=(n_cols, )))\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "#fit the model\n",
    "model.fit(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('model_file.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 418\n",
      "Trainable params: 418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "my_model = load_model('model_file.h5')\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model optimization\n",
    "- **Dying neuron** problem:\n",
    "    - Once a node start always getting negative inputs, it may conitnue getting always neg value so contributing nothing\n",
    "- **vanishing** gradient:\n",
    "    - when many layers have very small slopes\n",
    "    - in case of DL, updates to backprob were close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape = (input_shape, )))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s 555us/step - loss: 4.1101 - acc: 0.6162\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 43us/step - loss: 4.1039 - acc: 0.6162\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 4.0977 - acc: 0.6162\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 4.0914 - acc: 0.6162\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 4.0850 - acc: 0.6162\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 4.0786 - acc: 0.6162\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 4.0722 - acc: 0.6162\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 4.0657 - acc: 0.6162\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 38us/step - loss: 4.0593 - acc: 0.6162\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 41us/step - loss: 4.0529 - acc: 0.6162\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 577us/step - loss: 2.2188 - acc: 0.5948\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.7616 - acc: 0.6521\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 41us/step - loss: 0.6836 - acc: 0.6498\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.6102 - acc: 0.6880\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.6268 - acc: 0.6655\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.5905 - acc: 0.6880\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 41us/step - loss: 0.5939 - acc: 0.6958\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.5887 - acc: 0.6902\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.5832 - acc: 0.7048\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.5812 - acc: 0.6981\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 611us/step - loss: 6.0233 - acc: 0.6083\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 44us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 43us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 43us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 45us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 41us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 39us/step - loss: 6.1867 - acc: 0.6162\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 41us/step - loss: 6.1867 - acc: 0.6162\n"
     ]
    }
   ],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "n_cols = X.shape[1]\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model(n_cols)\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X, target, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/10\n",
      "623/623 [==============================] - 0s 370us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 0s 54us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 0s 53us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 0s 58us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - 0s 56us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 6/10\n",
      "623/623 [==============================] - 0s 56us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 7/10\n",
      "623/623 [==============================] - 0s 53us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 8/10\n",
      "623/623 [==============================] - 0s 54us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 9/10\n",
      "623/623 [==============================] - 0s 53us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n",
      "Epoch 10/10\n",
      "623/623 [==============================] - 0s 54us/step - loss: 6.3644 - acc: 0.6051 - val_loss: 5.7736 - val_acc: 0.6418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e132ea0668>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, target, epochs=10, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/50\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 1.9112 - acc: 0.6340 - val_loss: 1.1935 - val_acc: 0.5672\n",
      "Epoch 2/50\n",
      "623/623 [==============================] - 0s 61us/step - loss: 1.0792 - acc: 0.6180 - val_loss: 0.7133 - val_acc: 0.6866\n",
      "Epoch 3/50\n",
      "623/623 [==============================] - 0s 59us/step - loss: 0.7101 - acc: 0.6260 - val_loss: 0.5848 - val_acc: 0.7351\n",
      "Epoch 4/50\n",
      "623/623 [==============================] - 0s 59us/step - loss: 0.6329 - acc: 0.6758 - val_loss: 0.5671 - val_acc: 0.7351\n",
      "Epoch 5/50\n",
      "623/623 [==============================] - 0s 56us/step - loss: 0.6049 - acc: 0.6854 - val_loss: 0.5358 - val_acc: 0.7649\n",
      "Epoch 6/50\n",
      "623/623 [==============================] - 0s 56us/step - loss: 0.5798 - acc: 0.6982 - val_loss: 0.5640 - val_acc: 0.6940\n",
      "Epoch 7/50\n",
      "623/623 [==============================] - 0s 59us/step - loss: 0.5648 - acc: 0.7111 - val_loss: 0.5039 - val_acc: 0.7649\n",
      "Epoch 8/50\n",
      "623/623 [==============================] - 0s 58us/step - loss: 0.5664 - acc: 0.7207 - val_loss: 0.5034 - val_acc: 0.7425\n",
      "Epoch 9/50\n",
      "623/623 [==============================] - 0s 56us/step - loss: 0.5523 - acc: 0.7271 - val_loss: 0.5474 - val_acc: 0.7090\n",
      "Epoch 10/50\n",
      "623/623 [==============================] - 0s 56us/step - loss: 0.5647 - acc: 0.7030 - val_loss: 0.4886 - val_acc: 0.7910\n",
      "Epoch 11/50\n",
      "623/623 [==============================] - 0s 58us/step - loss: 0.5628 - acc: 0.7512 - val_loss: 0.4968 - val_acc: 0.7948\n",
      "Epoch 12/50\n",
      "623/623 [==============================] - 0s 59us/step - loss: 0.5155 - acc: 0.7753 - val_loss: 0.4649 - val_acc: 0.7761\n",
      "Epoch 13/50\n",
      "623/623 [==============================] - 0s 59us/step - loss: 0.5303 - acc: 0.7592 - val_loss: 0.4524 - val_acc: 0.7799\n",
      "Epoch 14/50\n",
      "623/623 [==============================] - 0s 58us/step - loss: 0.4918 - acc: 0.7705 - val_loss: 0.4761 - val_acc: 0.7985\n",
      "Epoch 15/50\n",
      "623/623 [==============================] - 0s 58us/step - loss: 0.4980 - acc: 0.7833 - val_loss: 0.4553 - val_acc: 0.7799\n",
      "Epoch 16/50\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.4154 - acc: 0.781 - 0s 61us/step - loss: 0.4913 - acc: 0.7801 - val_loss: 0.4596 - val_acc: 0.7873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1499ae550>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Early stopping\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#how many epochs the model can go without improving before we stop training, 2 or 3 (more it is unlikly that model will improve)\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols, )))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, target, validation_split=0.3, epochs=50, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('classifier_nn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about model capacity\n",
    "<img src=\"./graphs/model_complexity_error_training_test.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping up to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://assets.datacamp.com/production/course_1975/datasets/mnist.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = to_categorical(df[df.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 601 samples\n",
      "Epoch 1/10000\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 9.4226 - acc: 0.3807 - val_loss: 8.0268 - val_acc: 0.4809\n",
      "Epoch 2/10000\n",
      "1400/1400 [==============================] - 0s 121us/step - loss: 7.8286 - acc: 0.4964 - val_loss: 7.3280 - val_acc: 0.5308\n",
      "Epoch 3/10000\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 7.0424 - acc: 0.5514 - val_loss: 7.5103 - val_acc: 0.5208\n",
      "Epoch 4/10000\n",
      "1400/1400 [==============================] - 0s 118us/step - loss: 7.0196 - acc: 0.5571 - val_loss: 7.5732 - val_acc: 0.5191\n",
      "Epoch 5/10000\n",
      "1400/1400 [==============================] - 0s 131us/step - loss: 6.9719 - acc: 0.5614 - val_loss: 7.5742 - val_acc: 0.5275\n",
      "Epoch 6/10000\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 6.8024 - acc: 0.5686 - val_loss: 6.8117 - val_acc: 0.5674\n",
      "Epoch 7/10000\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 6.5456 - acc: 0.5836 - val_loss: 6.0324 - val_acc: 0.6073\n",
      "Epoch 8/10000\n",
      "1400/1400 [==============================] - 0s 131us/step - loss: 5.8498 - acc: 0.6193 - val_loss: 5.7691 - val_acc: 0.6290\n",
      "Epoch 9/10000\n",
      "1400/1400 [==============================] - 0s 127us/step - loss: 5.3613 - acc: 0.6529 - val_loss: 5.1853 - val_acc: 0.6639\n",
      "Epoch 10/10000\n",
      "1400/1400 [==============================] - 0s 135us/step - loss: 5.1109 - acc: 0.6693 - val_loss: 4.6144 - val_acc: 0.7005\n",
      "Epoch 11/10000\n",
      "1400/1400 [==============================] - 0s 133us/step - loss: 5.0774 - acc: 0.6743 - val_loss: 5.2296 - val_acc: 0.6689\n",
      "Epoch 12/10000\n",
      "1400/1400 [==============================] - 0s 127us/step - loss: 4.3974 - acc: 0.7186 - val_loss: 3.7122 - val_acc: 0.7621\n",
      "Epoch 13/10000\n",
      "1400/1400 [==============================] - 0s 124us/step - loss: 3.8075 - acc: 0.7493 - val_loss: 4.0431 - val_acc: 0.7321\n",
      "Epoch 14/10000\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 3.7285 - acc: 0.7593 - val_loss: 4.1765 - val_acc: 0.7321\n",
      "Epoch 15/10000\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 3.5199 - acc: 0.7714 - val_loss: 3.9867 - val_acc: 0.7371\n",
      "Epoch 16/10000\n",
      "1400/1400 [==============================] - 0s 125us/step - loss: 3.3759 - acc: 0.7814 - val_loss: 3.8946 - val_acc: 0.7488\n",
      "Epoch 17/10000\n",
      "1400/1400 [==============================] - 0s 123us/step - loss: 3.1762 - acc: 0.7971 - val_loss: 3.6173 - val_acc: 0.7687\n",
      "Epoch 18/10000\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 3.0753 - acc: 0.8029 - val_loss: 3.5927 - val_acc: 0.7671\n",
      "Epoch 19/10000\n",
      "1400/1400 [==============================] - 0s 123us/step - loss: 3.3459 - acc: 0.7843 - val_loss: 4.6653 - val_acc: 0.7005\n",
      "Epoch 20/10000\n",
      "1400/1400 [==============================] - 0s 123us/step - loss: 3.3052 - acc: 0.7879 - val_loss: 3.8974 - val_acc: 0.7454\n",
      "Epoch 21/10000\n",
      "1400/1400 [==============================] - 0s 129us/step - loss: 3.3115 - acc: 0.7886 - val_loss: 3.9630 - val_acc: 0.7421\n",
      "Epoch 22/10000\n",
      "1400/1400 [==============================] - 0s 121us/step - loss: 3.1966 - acc: 0.7957 - val_loss: 3.2637 - val_acc: 0.7887\n",
      "Epoch 23/10000\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 2.9107 - acc: 0.8114 - val_loss: 3.2094 - val_acc: 0.7920\n",
      "Epoch 24/10000\n",
      "1400/1400 [==============================] - 0s 118us/step - loss: 3.0251 - acc: 0.8057 - val_loss: 3.3082 - val_acc: 0.7837\n",
      "Epoch 25/10000\n",
      "1400/1400 [==============================] - 0s 127us/step - loss: 2.9295 - acc: 0.8129 - val_loss: 3.3625 - val_acc: 0.7837\n",
      "Epoch 26/10000\n",
      "1400/1400 [==============================] - 0s 131us/step - loss: 3.0907 - acc: 0.8057 - val_loss: 3.3790 - val_acc: 0.7820\n",
      "Epoch 27/10000\n",
      "1400/1400 [==============================] - 0s 121us/step - loss: 2.8871 - acc: 0.8129 - val_loss: 3.5046 - val_acc: 0.7704\n",
      "Epoch 28/10000\n",
      "1400/1400 [==============================] - 0s 123us/step - loss: 3.2420 - acc: 0.7936 - val_loss: 4.0113 - val_acc: 0.7421\n",
      "Epoch 29/10000\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 2.9907 - acc: 0.8079 - val_loss: 3.3861 - val_acc: 0.7820\n",
      "Epoch 30/10000\n",
      "1400/1400 [==============================] - 0s 139us/step - loss: 2.8391 - acc: 0.8200 - val_loss: 3.3213 - val_acc: 0.7837\n",
      "Epoch 31/10000\n",
      "1400/1400 [==============================] - 0s 125us/step - loss: 2.6868 - acc: 0.8279 - val_loss: 3.2698 - val_acc: 0.7887\n",
      "Epoch 32/10000\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 2.8259 - acc: 0.8179 - val_loss: 3.3347 - val_acc: 0.7854\n",
      "Epoch 33/10000\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 2.6626 - acc: 0.8314 - val_loss: 3.0661 - val_acc: 0.8053\n",
      "Epoch 34/10000\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 2.5907 - acc: 0.8343 - val_loss: 2.8468 - val_acc: 0.8103\n",
      "Epoch 35/10000\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 2.5337 - acc: 0.8400 - val_loss: 2.9830 - val_acc: 0.8120\n",
      "Epoch 36/10000\n",
      "1400/1400 [==============================] - 0s 125us/step - loss: 2.4652 - acc: 0.8443 - val_loss: 3.3858 - val_acc: 0.7837\n",
      "Epoch 37/10000\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 2.7629 - acc: 0.8229 - val_loss: 3.4471 - val_acc: 0.7787\n",
      "Epoch 38/10000\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 2.7662 - acc: 0.8250 - val_loss: 3.1160 - val_acc: 0.7937\n",
      "Epoch 39/10000\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 2.6457 - acc: 0.8329 - val_loss: 2.9033 - val_acc: 0.8120\n",
      "Epoch 40/10000\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 2.4874 - acc: 0.8443 - val_loss: 3.1174 - val_acc: 0.7953\n",
      "Epoch 41/10000\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 2.6785 - acc: 0.8279 - val_loss: 3.2898 - val_acc: 0.7887\n",
      "Epoch 42/10000\n",
      "1400/1400 [==============================] - 0s 124us/step - loss: 2.6528 - acc: 0.8307 - val_loss: 3.0594 - val_acc: 0.8020\n",
      "Epoch 43/10000\n",
      "1400/1400 [==============================] - 0s 112us/step - loss: 2.6245 - acc: 0.8336 - val_loss: 3.3838 - val_acc: 0.7820\n",
      "Epoch 44/10000\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 2.5672 - acc: 0.8371 - val_loss: 3.1645 - val_acc: 0.7970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e14ce5fe48>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu',  input_shape=(784, )))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=10000, validation_split=0.3, callbacks=[early_stopping_monitor])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
